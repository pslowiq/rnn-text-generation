{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sEKxB89FnH7V"
   },
   "source": [
    "# Starting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U3MZmBDEnbUD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhM14WQ2nMgW"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9-ANnENm_f-",
    "outputId": "7fa75690-6cc6-4f54-acfe-3a165088f768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 39 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open(\"input.txt\", \"r\", encoding=\"UTF-8\").read()\n",
    "data = data.lower()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "str_to_idx = lambda x: list(map(lambda xx: char_to_idx[xx], x))\n",
    "idx_to_str = lambda x: \"\".join(list(map(lambda xx: idx_to_char[xx], x)))\n",
    "tensor_to_str = lambda x: \"\".join(list(map(lambda xx: idx_to_char[xx.item()], x)))\n",
    "\n",
    "\n",
    "def str_to_tensor(x, device=\"cpu\"):\n",
    "    return torch.tensor(list(map(lambda xx: char_to_idx[xx], x)), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3DvMxP9yAsBs"
   },
   "outputs": [],
   "source": [
    "class TextSampler(Dataset):\n",
    "    def __init__(self, data, seq_size):\n",
    "        self.seq_size = seq_size + 1\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gen = str_to_idx(\n",
    "            self.data[idx * self.seq_size : idx * self.seq_size + self.seq_size]\n",
    "        )\n",
    "        x = np.array(gen[:-1], dtype=int)\n",
    "        y = np.array(gen[1:], dtype=int)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TcS60-vwAs5d"
   },
   "source": [
    "# Vanilla RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "15Pd8CjBr1Kk"
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(L.LightningModule):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = nn.Parameter(\n",
    "            torch.normal(0, 1, size=(hidden_size, vocab_size)) * 0.01\n",
    "        )\n",
    "        self.Whh = nn.Parameter(\n",
    "            torch.normal(0, 1, size=(hidden_size, hidden_size)) * 0.01\n",
    "        )\n",
    "        self.bh = nn.Parameter(torch.zeros(size=(hidden_size, 1), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, h_prev=None):\n",
    "        seq_length, _ = x.shape\n",
    "        x = torch.unsqueeze(x, dim=-1)\n",
    "\n",
    "        hiddens = torch.zeros(\n",
    "            size=(seq_length, self.hidden_size, 1),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            h_prev = nn.LeakyReLU()(self.Wxh @ x[t] + self.Whh @ h_prev + self.bh)\n",
    "            hiddens[t] = h_prev\n",
    "\n",
    "        return hiddens, hiddens[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "djnMgJ-NWydk"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.U = nn.ParameterList(\n",
    "            [\n",
    "                torch.normal(0, 1, size=(hidden_size, vocab_size)) * 0.01\n",
    "                for i in range(4)\n",
    "            ]\n",
    "        )\n",
    "        self.W = nn.ParameterList(\n",
    "            [\n",
    "                torch.normal(0, 1, size=(hidden_size, hidden_size)) * 0.01\n",
    "                for i in range(4)\n",
    "            ]\n",
    "        )\n",
    "        self.b = nn.ParameterList(\n",
    "            [torch.zeros(size=(hidden_size, 1), dtype=torch.float32) for i in range(4)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        Wi, Wf, Wo, Wc = self.W\n",
    "        Ui, Uf, Uo, Uc = self.U\n",
    "        bi, bf, bo, bc = self.b\n",
    "\n",
    "        seq_length, _ = x.shape\n",
    "        x = torch.unsqueeze(x, dim=-1)\n",
    "\n",
    "        hiddens = torch.zeros(\n",
    "            size=(seq_length, self.hidden_size, 1),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        cells = torch.zeros(\n",
    "            size=(seq_length, self.hidden_size, 1),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        h_prev, c_prev = state\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            F = nn.Sigmoid()(Uf @ x[t] + Wf @ h_prev + bf)\n",
    "            I = nn.Sigmoid()(Ui @ x[t] + Wi @ h_prev + bi)\n",
    "            O = nn.Sigmoid()(Uo @ x[t] + Wo @ h_prev + bo)\n",
    "            C = F * c_prev + I * nn.Tanh()(Uc @ x[t] + Wc @ h_prev + bc)\n",
    "            H = O * nn.Tanh()(C)\n",
    "            hiddens[t], cells[t] = H, C\n",
    "            h_prev, c_prev = H, C\n",
    "\n",
    "        return hiddens, torch.stack([hiddens[-1], cells[-1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "691e6d89addf4e4e98af6f715de9e270"
     ]
    },
    "id": "AkOftRS4Wvjk",
    "outputId": "42ae5b55-f82e-41f5-aa57-542382c13081"
   },
   "outputs": [],
   "source": [
    "class ShakespheareGenerator(L.LightningModule):\n",
    "    def __init__(self, recurrent_layer, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.train_loss = []\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.rnn = recurrent_layer(hidden_size, vocab_size)\n",
    "        self.dense = nn.Linear(hidden_size, vocab_size, bias=True)\n",
    "\n",
    "        self.hidden_state = 0\n",
    "        self.reset_hidden()\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        hiddens, state = self.rnn(x, state)\n",
    "        out = self.dense(hiddens.squeeze(dim=-1))\n",
    "\n",
    "        return out, state\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x[0], y[0]  # remove batch\n",
    "        x = F.one_hot(x, self.vocab_size).float()\n",
    "        out, h_prev = self.forward(x, self.hidden_state.clone().detach())\n",
    "        self.hidden_state = h_prev.clone().detach()\n",
    "        loss = nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "        self.train_loss.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        if type(self.rnn) == VanillaRNN:\n",
    "            self.hidden_state = torch.zeros(\n",
    "                size=(self.hidden_size, 1), dtype=torch.float32, device=self.device\n",
    "            )\n",
    "        elif type(self.rnn) == LSTM:\n",
    "            self.hidden_state = torch.stack(\n",
    "                [\n",
    "                    torch.zeros(\n",
    "                        size=(self.hidden_size, 1),\n",
    "                        dtype=torch.float32,\n",
    "                        device=self.device,\n",
    "                    ),\n",
    "                    torch.zeros(\n",
    "                        size=(self.hidden_size, 1),\n",
    "                        dtype=torch.float32,\n",
    "                        device=self.device,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.reset_hidden()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_acc = sum(self.train_loss) / len(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        self.reset_hidden()\n",
    "        self.print(f\"Epoch {self.current_epoch} Training Loss: {avg_train_acc:.5f}\")\n",
    "        print(\"------------------------------------\")\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"duke vincentio:\"))\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    def generate(self, start=\"d\", max_len=100):\n",
    "        ret = start\n",
    "\n",
    "        self.reset_hidden()\n",
    "        last_hidden = self.hidden_state\n",
    "\n",
    "        # accelerate the generation with starting sequence\n",
    "        input = F.one_hot(\n",
    "            str_to_tensor(start, device=self.device), self.vocab_size\n",
    "        ).float()\n",
    "        out, last_hidden = self.forward(input, last_hidden)\n",
    "        out = torch.softmax(out, dim=-1)[-1]\n",
    "        next = torch.multinomial(out, num_samples=1).squeeze()\n",
    "        input = F.one_hot(next, vocab_size).float()\n",
    "        ret = ret + idx_to_char[next.item()]\n",
    "\n",
    "        # then generate the rest 1 char by 1 char using previous hiddens\n",
    "        for t in range(max_len):\n",
    "            input = F.one_hot(\n",
    "                str_to_tensor(ret[-1], device=self.device), self.vocab_size\n",
    "            ).float()\n",
    "            out, last_hidden = self.forward(input, last_hidden)\n",
    "            out = torch.softmax(out, dim=-1)\n",
    "            next = torch.multinomial(out, num_samples=1).squeeze()\n",
    "            input = F.one_hot(next, vocab_size).float()\n",
    "            ret = ret + idx_to_char[next.item()]\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | rnn   | VanillaRNN | 21.5 K\n",
      "1 | dense | Linear     | 5.0 K \n",
      "-------------------------------------\n",
      "26.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.5 K    Total params\n",
      "0.106     Total estimated model params size (MB)\n",
      "/home/pslowiq/programs/shakespheare-text-generating/Lightning/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935bf24936224699b9b73cbc6b507ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.90169\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "i dout poen othe met'st; died? woulh non cone to are fargh rans, acome, which grief you asthing lild\n",
      "------------------------------------\n",
      "Epoch 1 Training Loss: 1.70002\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "thanks autist?\n",
      "\n",
      "sebastian:\n",
      "no dobless they's. i weaps. with being not more more ishid dell ol eldew'\n",
      "------------------------------------\n",
      "Epoch 2 Training Loss: 1.66014\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "to thancolm it them off, count the provirningst you dishormorat; and wathsef\n",
      "a son.\n",
      "\n",
      "antonio:\n",
      "sappon\n",
      "------------------------------------\n",
      "Epoch 3 Training Loss: 1.64097\n",
      "------------------------------------\n",
      "duke vincentio: to the broke?\n",
      "\n",
      "fortoyor wron all'd, whencinightly\n",
      "out they home to remofinast'd genslifuy got by rub\n",
      "------------------------------------\n",
      "Epoch 4 Training Loss: 1.62817\n",
      "------------------------------------\n",
      "duke vincentio: that?\n",
      "\n",
      "sebastian:\n",
      "they time a minim, that slups, tht the diss that opt so loid, which strough one wh\n",
      "------------------------------------\n",
      "Epoch 5 Training Loss: 1.61948\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "sly too speak more not engo stroed,\n",
      "of her,\n",
      "in their ste; whom dispost, my eye:\n",
      "i weak'd us? prossin\n",
      "------------------------------------\n",
      "Epoch 6 Training Loss: 1.61319\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "is and wore\n",
      "widowprocking, he good i see a just wast.\n",
      "\n",
      "omaninio:\n",
      "go frient.\n",
      "\n",
      "oncomio:\n",
      "to himple, thi\n",
      "------------------------------------\n",
      "Epoch 7 Training Loss: 1.60780\n",
      "------------------------------------\n",
      "duke vincentio: all contrance! go, for never didpayed a waves.\n",
      "golf. thy hath, week!\n",
      "\n",
      "alonso:\n",
      "toou,\n",
      "halut the fle's \n",
      "------------------------------------\n",
      "Epoch 8 Training Loss: 1.60464\n",
      "------------------------------------\n",
      "duke vincentio: not of your mace you are so your exticap's stien?\n",
      "\n",
      "antonio:\n",
      "o for the things in siglies\n",
      "i will he do\n",
      "------------------------------------\n",
      "Epoch 9 Training Loss: 1.60172\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "sturn, as yet bosed that shill sure we spire, and gorsage gentlewen inluck the behill kath,--\n",
      "\n",
      "polon\n",
      "------------------------------------\n",
      "Epoch 10 Training Loss: 1.59878\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "but save\n",
      "sweeter-new ster fromanthis\n",
      "comit: thou wendle.\n",
      "\n",
      "antonio:\n",
      "are father maling\n",
      "as is comb, sur\n",
      "------------------------------------\n",
      "Epoch 11 Training Loss: 1.59649\n",
      "------------------------------------\n",
      "duke vincentio: pertant it is weal.\n",
      "\n",
      "sebastian:\n",
      "thou shear retilirt things went, al nightlan moneily for:\n",
      "'t be crem\n",
      "------------------------------------\n",
      "Epoch 12 Training Loss: 1.59465\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "why wore without and gon, in says out: an she must see villain't down with,\n",
      "mrashion or a lost\n",
      "kneel\n",
      "------------------------------------\n",
      "Epoch 13 Training Loss: 1.59708\n",
      "------------------------------------\n",
      "duke vincentio: and at you hould.\n",
      "\n",
      "sontenio:\n",
      "islow\n",
      "and lord.\n",
      "\n",
      "lond:\n",
      "and not have whilnish,\n",
      "all;\n",
      "who sir, a quist o' \n",
      "------------------------------------\n",
      "Epoch 14 Training Loss: 1.59741\n",
      "------------------------------------\n",
      "duke vincentio: speath i'ld causening then?\n",
      "better wathous compose wint not!\n",
      "where's in bift the created under\n",
      "streg\n",
      "------------------------------------\n",
      "Epoch 15 Training Loss: 1.59197\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "indernable hath?\n",
      "\n",
      "antonio:\n",
      "or so asks i peash say.\n",
      "\n",
      "gonzalo:\n",
      "bsealing\n",
      "ast wave my ox\n",
      "the servant as \n",
      "------------------------------------\n",
      "Epoch 16 Training Loss: 1.59050\n",
      "------------------------------------\n",
      "duke vincentio: i father are, but of jely, gone,\n",
      "for my bounding,\n",
      "good.\n",
      "\n",
      "sebastian:\n",
      "i should with a more son and him\n",
      "------------------------------------\n",
      "Epoch 17 Training Loss: 1.59018\n",
      "------------------------------------\n",
      "duke vincentio: wath thing. when i sin wirh i commen:\n",
      "when he wee rough. eauth i do.\n",
      "\n",
      "gonzalo:\n",
      "it shrw clard of want\n",
      "------------------------------------\n",
      "Epoch 18 Training Loss: 1.58903\n",
      "------------------------------------\n",
      "duke vincentio:; no, and me, by manlar, go nothing me that i.\n",
      "\n",
      "antonso:\n",
      "madat the honoury,\n",
      "\n",
      "lonis trune, gollow thn \n",
      "------------------------------------\n",
      "Epoch 19 Training Loss: 1.58886\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "you uther not of bluss us minoany romostie.\n",
      "\n",
      "ponsonalo:\n",
      "how foul go unamboent, us, to turbst!\n",
      "and si\n",
      "------------------------------------\n",
      "Epoch 20 Training Loss: 1.58941\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "mistress, ala them,\n",
      "jorfessares; of ousages\n",
      "the son. help is stomped and kep umex find.\n",
      "\n",
      "antonio:\n",
      "my\n",
      "------------------------------------\n",
      "Epoch 21 Training Loss: 1.58739\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "had mithout in hither.\n",
      "\n",
      "gonzalo:\n",
      "in herd, but his\n",
      "word, so montact\n",
      "to be him.\n",
      "\n",
      "petruas:\n",
      "mistress, si\n",
      "------------------------------------\n",
      "Epoch 22 Training Loss: 1.58684\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "vilater beloress of my subt together, same you.\n",
      "\n",
      "antonio:\n",
      "thou spores, whose diler, faith, bring son\n",
      "------------------------------------\n",
      "Epoch 23 Training Loss: 1.58786\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "fain you had sail a forbeo speak incle staying. you in thee abiy?\n",
      "\n",
      "antonio:\n",
      "i wision his vogt ad yet\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training Loss: 1.58567\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "then do i we say, companched thoo, his will wed\n",
      "are: and my father the son, whiloul she is to cravin\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "seq_size = 40\n",
    "\n",
    "model = ShakespheareGenerator(VanillaRNN, hidden_size, vocab_size)\n",
    "\n",
    "train_dataset = TextSampler(data, seq_size=seq_size)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=25, accelerator=\"gpu\")\n",
    "trainer.fit(model, train_loader)\n",
    "\n",
    "with open(\"rnn_output.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"duke vincentio:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"romeo:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"juliet:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"a\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADCRq-y3zCdz",
    "outputId": "027524db-16ca-46a8-c5cf-65482a1979ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | rnn   | LSTM   | 86.0 K\n",
      "1 | dense | Linear | 5.0 K \n",
      "---------------------------------\n",
      "91.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "91.0 K    Total params\n",
      "0.364     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47724c8d5a9a47db978c5d071dc2b34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.83613\n",
      "------------------------------------\n",
      "duke vincentio: there my lough\n",
      "urmosterly pleastily as\n",
      "thay soke i'll breaken!' thou. not be\n",
      "vingi\n",
      "age old less, as \n",
      "------------------------------------\n",
      "Epoch 1 Training Loss: 1.57221\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "good life she dith wanworl men, to-katishes.\n",
      "\n",
      "gonzalo:\n",
      "indoth! what? you wam so persent?\n",
      "\n",
      "alonso:\n",
      "yo\n",
      "------------------------------------\n",
      "Epoch 2 Training Loss: 1.50511\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "o honourn contuban; besevives it\n",
      "jest om enour but offends'\n",
      "so so, let my maps'dle him!\n",
      "\n",
      "aliustio:\n",
      "a\n",
      "------------------------------------\n",
      "Epoch 3 Training Loss: 1.47096\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "he gentlewonest embrace;\n",
      "and person of the down'd awad me; my book.\n",
      "\n",
      "gonzalo:\n",
      "now i dread the world \n",
      "------------------------------------\n",
      "Epoch 4 Training Loss: 1.44941\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "to proverse been spirits the more.\n",
      "\n",
      "gonzalo:\n",
      "that i tradiage with be chosent,\n",
      "a pridous, sir, untalk\n",
      "------------------------------------\n",
      "Epoch 5 Training Loss: 1.43423\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "all you shall othatabion forguiloccory:\n",
      "or to quit what feech them, wath'd baschis.\n",
      "\n",
      "pospertila:\n",
      "his\n",
      "------------------------------------\n",
      "Epoch 6 Training Loss: 1.42298\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "whon her, my house, 'tis man because sight we am\n",
      "\n",
      "triua:\n",
      "no, weak it! he should lost up. an arrest i\n",
      "------------------------------------\n",
      "Epoch 7 Training Loss: 1.41435\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "he standing and seen so is for my sore isle,\n",
      "an once afoldist.\n",
      "if mine entreat toathateman. we will \n",
      "------------------------------------\n",
      "Epoch 8 Training Loss: 1.40740\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "know. speak son, sureworthessery drunk\n",
      "ngolinat hither: i close this kneel to-mar\n",
      "wish quarrel.\n",
      "\n",
      "mir\n",
      "------------------------------------\n",
      "Epoch 9 Training Loss: 1.40175\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "goothear\n",
      "son, or being itlor; child, profalty;\n",
      "unto would prosper think in you.\n",
      "\n",
      "warinous:\n",
      "nay! meth\n",
      "------------------------------------\n",
      "Epoch 10 Training Loss: 1.39684\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "say't yet.\n",
      "what, so, but yet severy my sobse.\n",
      "we'll yet peace, as for me:\n",
      "eats of thou dapers and bl\n",
      "------------------------------------\n",
      "Epoch 11 Training Loss: 1.39309\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "under penf the save innourines!\n",
      "\n",
      "miranda:\n",
      "the vantood, would did since the creand\n",
      "by a swift and ask\n",
      "------------------------------------\n",
      "Epoch 12 Training Loss: 1.38918\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "so i are his brishes rome for withful sapot\n",
      "hou most aghne.\n",
      "\n",
      "gonzalo:\n",
      "who wish's it?\n",
      "\n",
      "ingnly:\n",
      "rust, \n",
      "------------------------------------\n",
      "Epoch 13 Training Loss: 1.38651\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "i think you're to present fair pression dgod\n",
      "to bartland him, 'tis thee that we\n",
      "speak good offectio?\n",
      "------------------------------------\n",
      "Epoch 14 Training Loss: 1.38400\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "doot!\n",
      "\n",
      "antonio:\n",
      "paran, or well.\n",
      "\n",
      "antonio:\n",
      "i'll late, foolish\n",
      "and name; so loye it of no inquicy.\n",
      "\n",
      "fi\n",
      "------------------------------------\n",
      "Epoch 15 Training Loss: 1.38176\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "twertain us? aria, nor our rablion.\n",
      "vouckned's their spirit\n",
      "were not flartheider'd at the most swail\n",
      "------------------------------------\n",
      "Epoch 16 Training Loss: 1.37968\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "every ricks against the sake.\n",
      "\n",
      "first mercian:\n",
      "lo your majesty:\n",
      "stay our life:\n",
      "i am minutes steaf. bu\n",
      "------------------------------------\n",
      "Epoch 17 Training Loss: 1.37793\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "that thy sown shallse rest; and these was.\n",
      "was you here way so forth, from the hands of norfolk do's\n",
      "------------------------------------\n",
      "Epoch 18 Training Loss: 1.37604\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "and kind unto there, with a best;\n",
      "wherein my duries finger debalue.\n",
      "\n",
      "prospero:\n",
      "yet they think god ra\n",
      "------------------------------------\n",
      "Epoch 19 Training Loss: 1.37455\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "what be your tresple. she brass and true;\n",
      "the glien and nistresh-mere and weak and hit.\n",
      "\n",
      "antonio:\n",
      "wh\n",
      "------------------------------------\n",
      "Epoch 20 Training Loss: 1.37321\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "and ives him: be sister here in this body:\n",
      "what is now we, some, would.\n",
      "\n",
      "miranda:\n",
      "sir, lodging name,\n",
      "------------------------------------\n",
      "Epoch 21 Training Loss: 1.37175\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "ay, and learn'st bring'st might said 'twas left it with most\n",
      "on her isabel,'send we watch'd,\n",
      "serves \n",
      "------------------------------------\n",
      "Epoch 22 Training Loss: 1.37058\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "what sightso, when she should wonted and content\n",
      "unton here; for user-were.\n",
      "\n",
      "isania:\n",
      "no, would ha wa\n",
      "------------------------------------\n",
      "Epoch 23 Training Loss: 1.36936\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "i am all.\n",
      "\n",
      "antonio:\n",
      "ho, you should se us but love;\n",
      "what he make redience: i spieds me take,\n",
      "hirst i \n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training Loss: 1.36831\n",
      "------------------------------------\n",
      "duke vincentio:\n",
      "nothing brother of a dogat out sluck words!\n",
      "\n",
      "petruchio:\n",
      "or thine sister?\n",
      "\n",
      "lucento:\n",
      "hast thou hear th\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "seq_size = 40\n",
    "\n",
    "model = ShakespheareGenerator(LSTM, hidden_size, vocab_size)\n",
    "\n",
    "train_dataset = TextSampler(data, seq_size=seq_size)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=False\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=25, accelerator=\"gpu\")\n",
    "trainer.fit(model, train_loader)\n",
    "\n",
    "model.generate(start=\"duke vincentio:\", max_len=500)\n",
    "\n",
    "with open(\"lstm_output.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"duke vincentio:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"romeo:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"juliet:\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)\n",
    "    print(model.generate(start=\"a\", max_len=1000), file=f)\n",
    "    print(\"---------------------------------------------\", file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "sEKxB89FnH7V"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "25844c707ca746138e2e301733e319bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_295369647b5f48d5be1fc0d4315c834d",
      "max": 784,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f25e6d7c23d949f9a3e29bcdc4312f19",
      "value": 480
     }
    },
    "295369647b5f48d5be1fc0d4315c834d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a5fed2d98c949818b136cd90102f67b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ebcdb902fc5483ba67ef314c0ee5cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47d11c1457114d20a0a8d5a4f9250561",
       "IPY_MODEL_25844c707ca746138e2e301733e319bc",
       "IPY_MODEL_b70656e2475b478e992e9530c34a35e3"
      ],
      "layout": "IPY_MODEL_e5373cd5da254bd38d85127cee89fdf9"
     }
    },
    "47d11c1457114d20a0a8d5a4f9250561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a5fed2d98c949818b136cd90102f67b",
      "placeholder": "​",
      "style": "IPY_MODEL_f389c662c24546fc87316ef56bf65955",
      "value": "Epoch 45:  61%"
     }
    },
    "590a7afda2fd4952a0d651db7be57b80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b70656e2475b478e992e9530c34a35e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c00c8f4bf54c4a31898c345b83144d69",
      "placeholder": "​",
      "style": "IPY_MODEL_590a7afda2fd4952a0d651db7be57b80",
      "value": " 480/784 [00:10&lt;00:06, 44.58it/s, v_num=3]"
     }
    },
    "c00c8f4bf54c4a31898c345b83144d69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5373cd5da254bd38d85127cee89fdf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "f25e6d7c23d949f9a3e29bcdc4312f19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f389c662c24546fc87316ef56bf65955": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
