{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sEKxB89FnH7V"
   },
   "source": [
    "# Starting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "U3MZmBDEnbUD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhM14WQ2nMgW"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9-ANnENm_f-",
    "outputId": "00cde6f2-9aa6-4901-a3ae-4c8a768cc2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 5 characters, 5 unique.\n"
     ]
    }
   ],
   "source": [
    "data = \"ABCD#\"\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(data)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(data)}\n",
    "str_to_idx = lambda x: list(map(lambda xx: char_to_idx[xx], x))\n",
    "idx_to_str = lambda x: \"\".join(list(map(lambda xx: idx_to_char[xx], x)))\n",
    "tensor_to_str = lambda x: \"\".join(list(map(lambda xx: idx_to_char[xx.item()], x)))\n",
    "\n",
    "\n",
    "def str_to_tensor(x, device=\"cpu\"):\n",
    "    return torch.tensor(list(map(lambda xx: char_to_idx[xx], x)), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKqbc9FEnRUU",
    "outputId": "7c7601f0-0dc7-4b06-f013-ac104bb42e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'C', 'D', 'B', 'A']\n",
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, '#': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 2, 3, 1, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chars)\n",
    "print(char_to_idx)\n",
    "str_to_idx(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkcKdO06nZJj",
    "outputId": "0aa35490-05de-469e-c51a-4758b1268d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 4 tensor([0, 0, 0, 0, 1])\n",
      "C 2 tensor([0, 0, 1, 0, 0])\n",
      "D 3 tensor([0, 0, 0, 1, 0])\n",
      "B 1 tensor([0, 1, 0, 0, 0])\n",
      "A 0 tensor([1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "idxes = str_to_idx(chars)\n",
    "idxes_oh = F.one_hot(torch.tensor(idxes), vocab_size)\n",
    "\n",
    "for char, idx, idx_oh in zip(chars, idxes, idxes_oh):\n",
    "    print(char, idx, idx_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HZR1fhqppBnS",
    "outputId": "b42c4ab8-4328-4487-d382-3d81f445fe2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DCABCAABBBCABBCD#'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_string(ending_pattern: str):\n",
    "    ret = torch.randint(0, vocab_size - 1, size=(torch.randint(5, 15, (1,)),))\n",
    "    ret = torch.concatenate([ret, torch.tensor(str_to_idx(ending_pattern + \"#\"))])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "tensor_to_str(random_string(\"ABBCD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3DvMxP9yAsBs"
   },
   "outputs": [],
   "source": [
    "class TextSampler(Dataset):\n",
    "    def __init__(self, pattern, vocab_size):\n",
    "        self.sample_size = 128\n",
    "        self.pattern = pattern\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = random_string(self.pattern)\n",
    "        x = np.array(data[:-1], dtype=int)\n",
    "        y = np.array(data[1:], dtype=int)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcS60-vwAs5d"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15Pd8CjBr1Kk",
    "outputId": "fb214293-9c3f-4503-a40a-01e49d5cbef6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 7, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(L.LightningModule):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = nn.Parameter(\n",
    "            torch.normal(0, 1, size=(hidden_size, vocab_size)) * 0.01\n",
    "        )\n",
    "        self.Whh = nn.Parameter(\n",
    "            torch.normal(0, 1, size=(hidden_size, hidden_size)) * 0.01\n",
    "        )\n",
    "        self.bh = nn.Parameter(torch.zeros(size=(hidden_size, 1), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, h_prev=None):\n",
    "        seq_length, _ = x.shape\n",
    "        x = torch.unsqueeze(x, dim=-1)\n",
    "\n",
    "        out = torch.zeros(\n",
    "            size=(seq_length, self.hidden_size, 1),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        if h_prev == None:\n",
    "            h_prev = torch.zeros(\n",
    "                size=(self.hidden_size, 1), dtype=torch.float32, device=self.device\n",
    "            )\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            h_prev = nn.LeakyReLU()(self.Wxh @ x[t] + self.Whh @ h_prev + self.bh)\n",
    "            out[t] = h_prev\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "input = F.one_hot(random_string(\"ABBCD\"), vocab_size)\n",
    "\n",
    "RNN(7, vocab_size)(input.float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "42d6827efc05462d8acb60f29d90d849",
      "50d5f476be1148c4a9502c3247ee5b49",
      "6037cf11e7294a83bbb8a67268a2d106",
      "769836f3b05b4e5ebf8df1a08d7e6f54",
      "cf7687bfef6540b490cf183b0eeb4188",
      "6c76d17d4f86485b8db168d9a66b9f5a",
      "be09dfe1386b42c8a5a8e1027e11ed35",
      "ea3522dd3ef349f09bc19769078f7887",
      "3e980ecc0f8842689b3221a00718ecf8",
      "15c7f8ccc8574ea79f6070caf809b88b",
      "ee662183e28a4ad185379742c1ce5d09"
     ]
    },
    "id": "hjcCVnQ8ystp",
    "outputId": "ee9cc193-bb95-4e59-f65d-caad5a711455"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | rnn   | RNN    | 91    \n",
      "1 | dense | Linear | 40    \n",
      "---------------------------------\n",
      "131       Trainable params\n",
      "0         Non-trainable params\n",
      "131       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef54545d44142e49c7a04e5de86543d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.55319\n",
      "------------------------------------\n",
      "ADCCBCCCABCB#\n",
      "------------------------------------\n",
      "Epoch 1 Training Loss: 1.42898\n",
      "------------------------------------\n",
      "ADBBACADAADABACDABDBBBAAABBCD#\n",
      "------------------------------------\n",
      "Epoch 2 Training Loss: 1.28116\n",
      "------------------------------------\n",
      "ADCABCCDDABBDACBBBCDABBCCACBBCDAABADA#\n",
      "------------------------------------\n",
      "Epoch 3 Training Loss: 1.21343\n",
      "------------------------------------\n",
      "ABADD#\n",
      "------------------------------------\n",
      "Epoch 4 Training Loss: 1.17392\n",
      "------------------------------------\n",
      "ABBBDAA#\n",
      "------------------------------------\n",
      "Epoch 5 Training Loss: 1.16157\n",
      "------------------------------------\n",
      "AADBBCCDACBBBAA#\n",
      "------------------------------------\n",
      "Epoch 6 Training Loss: 1.14426\n",
      "------------------------------------\n",
      "AAADAABCDA#\n",
      "------------------------------------\n",
      "Epoch 7 Training Loss: 1.12554\n",
      "------------------------------------\n",
      "AAABCAABCDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 8 Training Loss: 1.08741\n",
      "------------------------------------\n",
      "ACDAABBCDAADAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 9 Training Loss: 1.07750\n",
      "------------------------------------\n",
      "ABCDDA#\n",
      "------------------------------------\n",
      "Epoch 10 Training Loss: 1.08420\n",
      "------------------------------------\n",
      "ADCBCDDDACCBBDABBBCDA#\n",
      "------------------------------------\n",
      "Epoch 11 Training Loss: 1.06414\n",
      "------------------------------------\n",
      "ADCBCACACDCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 12 Training Loss: 1.07590\n",
      "------------------------------------\n",
      "ADABBDADBBBCA#\n",
      "------------------------------------\n",
      "Epoch 13 Training Loss: 1.03241\n",
      "------------------------------------\n",
      "AABBCCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 14 Training Loss: 1.05033\n",
      "------------------------------------\n",
      "A#\n",
      "------------------------------------\n",
      "Epoch 15 Training Loss: 1.05589\n",
      "------------------------------------\n",
      "ACCBADABCADABACCADBBCDA#\n",
      "------------------------------------\n",
      "Epoch 16 Training Loss: 1.02856\n",
      "------------------------------------\n",
      "A#\n",
      "------------------------------------\n",
      "Epoch 17 Training Loss: 1.02752\n",
      "------------------------------------\n",
      "AABCDABCCA#\n",
      "------------------------------------\n",
      "Epoch 18 Training Loss: 1.04212\n",
      "------------------------------------\n",
      "ADABBBBDDBBBADAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 19 Training Loss: 1.02839\n",
      "------------------------------------\n",
      "AABDBCDBBBCCA#\n",
      "------------------------------------\n",
      "Epoch 20 Training Loss: 1.03501\n",
      "------------------------------------\n",
      "ADBBCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 21 Training Loss: 1.01962\n",
      "------------------------------------\n",
      "ACBCCCDACCBCDAADBBCDA#\n",
      "------------------------------------\n",
      "Epoch 22 Training Loss: 1.00748\n",
      "------------------------------------\n",
      "ADDABBBDA#\n",
      "------------------------------------\n",
      "Epoch 23 Training Loss: 0.97914\n",
      "------------------------------------\n",
      "ACADBBDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 24 Training Loss: 1.00489\n",
      "------------------------------------\n",
      "ABCCBACCCABBDA#\n",
      "------------------------------------\n",
      "Epoch 25 Training Loss: 1.00599\n",
      "------------------------------------\n",
      "ACCCDAACBDCDABBBDA#\n",
      "------------------------------------\n",
      "Epoch 26 Training Loss: 1.00525\n",
      "------------------------------------\n",
      "ACADCABBAAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 27 Training Loss: 0.99824\n",
      "------------------------------------\n",
      "ACDBCDBCABCBCDABBAAABBCAA#\n",
      "------------------------------------\n",
      "Epoch 28 Training Loss: 0.99174\n",
      "------------------------------------\n",
      "ACCDACACBDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 29 Training Loss: 0.98202\n",
      "------------------------------------\n",
      "AACDADACCDBADAABCDA#\n",
      "------------------------------------\n",
      "Epoch 30 Training Loss: 0.99080\n",
      "------------------------------------\n",
      "AABCBABBBCA#\n",
      "------------------------------------\n",
      "Epoch 31 Training Loss: 0.99292\n",
      "------------------------------------\n",
      "ADADADABBBCDA#\n",
      "------------------------------------\n",
      "Epoch 32 Training Loss: 0.99277\n",
      "------------------------------------\n",
      "ABCCBBBADAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 33 Training Loss: 0.99325\n",
      "------------------------------------\n",
      "ABABCDDCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 34 Training Loss: 0.98051\n",
      "------------------------------------\n",
      "ABCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 35 Training Loss: 0.97358\n",
      "------------------------------------\n",
      "ABDAAAABABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 36 Training Loss: 0.97311\n",
      "------------------------------------\n",
      "ABACCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 37 Training Loss: 0.98066\n",
      "------------------------------------\n",
      "ADAADBBDA#\n",
      "------------------------------------\n",
      "Epoch 38 Training Loss: 0.99241\n",
      "------------------------------------\n",
      "ADBDADBACBBBCDA#\n",
      "------------------------------------\n",
      "Epoch 39 Training Loss: 0.95805\n",
      "------------------------------------\n",
      "AABBABCAADBBCDA#\n",
      "------------------------------------\n",
      "Epoch 40 Training Loss: 0.96003\n",
      "------------------------------------\n",
      "ABAAABBDBCBCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 41 Training Loss: 0.97246\n",
      "------------------------------------\n",
      "ABCADBBDABCCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 42 Training Loss: 0.97700\n",
      "------------------------------------\n",
      "AADCCBAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 43 Training Loss: 0.97257\n",
      "------------------------------------\n",
      "ACAABCDADADBBBCA#\n",
      "------------------------------------\n",
      "Epoch 44 Training Loss: 0.96218\n",
      "------------------------------------\n",
      "ABBDABABBBCCA#\n",
      "------------------------------------\n",
      "Epoch 45 Training Loss: 0.96224\n",
      "------------------------------------\n",
      "ADAADBABBCDA#\n",
      "------------------------------------\n",
      "Epoch 46 Training Loss: 0.97715\n",
      "------------------------------------\n",
      "AAADBBCDABBBCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 47 Training Loss: 0.96648\n",
      "------------------------------------\n",
      "ABABBAABCDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 48 Training Loss: 0.96910\n",
      "------------------------------------\n",
      "ABDACDBBCDA#\n",
      "------------------------------------\n",
      "Epoch 49 Training Loss: 0.97306\n",
      "------------------------------------\n",
      "ABAABBBABCDABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 50 Training Loss: 0.97291\n",
      "------------------------------------\n",
      "ABDCBABCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 51 Training Loss: 0.97341\n",
      "------------------------------------\n",
      "ABBCCCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 52 Training Loss: 0.95083\n",
      "------------------------------------\n",
      "ABBBBCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 53 Training Loss: 0.95728\n",
      "------------------------------------\n",
      "ACACCDBDCDBBCAABBDABBCDABBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "------------------------------------\n",
      "Epoch 54 Training Loss: 0.94879\n",
      "------------------------------------\n",
      "ACDADDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 55 Training Loss: 0.97744\n",
      "------------------------------------\n",
      "ADCACAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 56 Training Loss: 0.96199\n",
      "------------------------------------\n",
      "AADBCDCADCADBCDBBBCDA#\n",
      "------------------------------------\n",
      "Epoch 57 Training Loss: 0.97556\n",
      "------------------------------------\n",
      "ADDBCCBABBCDA#\n",
      "------------------------------------\n",
      "Epoch 58 Training Loss: 0.96894\n",
      "------------------------------------\n",
      "ABAACAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 59 Training Loss: 0.95109\n",
      "------------------------------------\n",
      "ABDDBDCABCDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 60 Training Loss: 0.95682\n",
      "------------------------------------\n",
      "ACCACABCDABDCBBCDA#\n",
      "------------------------------------\n",
      "Epoch 61 Training Loss: 0.96045\n",
      "------------------------------------\n",
      "ABDDCCADCDAABCCA#\n",
      "------------------------------------\n",
      "Epoch 62 Training Loss: 0.95252\n",
      "------------------------------------\n",
      "ADADDBBBABBBCDA#\n",
      "------------------------------------\n",
      "Epoch 63 Training Loss: 0.93699\n",
      "------------------------------------\n",
      "ACDDCABBAABCCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 64 Training Loss: 0.95577\n",
      "------------------------------------\n",
      "ADBCADADCDBACBBCDA#\n",
      "------------------------------------\n",
      "Epoch 65 Training Loss: 0.96075\n",
      "------------------------------------\n",
      "ADBDBBBAAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 66 Training Loss: 0.96923\n",
      "------------------------------------\n",
      "ABBBBDDDACAADBCCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 67 Training Loss: 0.96397\n",
      "------------------------------------\n",
      "ABCBCADACABBABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 68 Training Loss: 0.93967\n",
      "------------------------------------\n",
      "ACDDABDBBBCDA#\n",
      "------------------------------------\n",
      "Epoch 69 Training Loss: 0.95694\n",
      "------------------------------------\n",
      "ACBBBBABBCDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 70 Training Loss: 0.94356\n",
      "------------------------------------\n",
      "ABCBBDDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 71 Training Loss: 0.94620\n",
      "------------------------------------\n",
      "ABBBCDABCDABBDDA#\n",
      "------------------------------------\n",
      "Epoch 72 Training Loss: 0.94729\n",
      "------------------------------------\n",
      "ACAACDDDADDDACABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 73 Training Loss: 0.97370\n",
      "------------------------------------\n",
      "ADBBCAACBABBCDA#\n",
      "------------------------------------\n",
      "Epoch 74 Training Loss: 0.94622\n",
      "------------------------------------\n",
      "ACBDDBDDDDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 75 Training Loss: 0.95509\n",
      "------------------------------------\n",
      "ABDCADABBCDA#\n",
      "------------------------------------\n",
      "Epoch 76 Training Loss: 0.92973\n",
      "------------------------------------\n",
      "ADDDCABCDABBBDA#\n",
      "------------------------------------\n",
      "Epoch 77 Training Loss: 0.94346\n",
      "------------------------------------\n",
      "ABAABBCAAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 78 Training Loss: 0.96563\n",
      "------------------------------------\n",
      "ABCAACBCBBABBCDA#\n",
      "------------------------------------\n",
      "Epoch 79 Training Loss: 0.96360\n",
      "------------------------------------\n",
      "ACBDABBABCCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 80 Training Loss: 0.95158\n",
      "------------------------------------\n",
      "ABCCDBBCDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 81 Training Loss: 0.93383\n",
      "------------------------------------\n",
      "ABBCCDABBCDDBBCDA#\n",
      "------------------------------------\n",
      "Epoch 82 Training Loss: 0.94318\n",
      "------------------------------------\n",
      "ADDDCCDCBABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 83 Training Loss: 0.95472\n",
      "------------------------------------\n",
      "AADDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 84 Training Loss: 0.93428\n",
      "------------------------------------\n",
      "ADBBDCABABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 85 Training Loss: 0.95033\n",
      "------------------------------------\n",
      "ACACBBCDBBABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 86 Training Loss: 0.94476\n",
      "------------------------------------\n",
      "ACDCACBBBCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 87 Training Loss: 0.94279\n",
      "------------------------------------\n",
      "ABCCABCDAAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 88 Training Loss: 0.94298\n",
      "------------------------------------\n",
      "ACCBDBBCADBBCCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 89 Training Loss: 0.94776\n",
      "------------------------------------\n",
      "ABCBBCABBCDA#\n",
      "------------------------------------\n",
      "Epoch 90 Training Loss: 0.95168\n",
      "------------------------------------\n",
      "ABBACBBCAADCCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 91 Training Loss: 0.93735\n",
      "------------------------------------\n",
      "A#\n",
      "------------------------------------\n",
      "Epoch 92 Training Loss: 0.93212\n",
      "------------------------------------\n",
      "ADDBADCBBABBCDA#\n",
      "------------------------------------\n",
      "Epoch 93 Training Loss: 0.94894\n",
      "------------------------------------\n",
      "AAAA#\n",
      "------------------------------------\n",
      "Epoch 94 Training Loss: 0.94879\n",
      "------------------------------------\n",
      "ACDACDABCAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 95 Training Loss: 0.95897\n",
      "------------------------------------\n",
      "ABBDADBBBCBBCDABBCDA#\n",
      "------------------------------------\n",
      "Epoch 96 Training Loss: 0.94476\n",
      "------------------------------------\n",
      "ADCADABAABABBCDA#\n",
      "------------------------------------\n",
      "Epoch 97 Training Loss: 0.93332\n",
      "------------------------------------\n",
      "ADABCDBBBDAABBCDA#\n",
      "------------------------------------\n",
      "Epoch 98 Training Loss: 0.96006\n",
      "------------------------------------\n",
      "AABCBBBCDAABBCDA#\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Training Loss: 0.92316\n",
      "------------------------------------\n",
      "ABABBCAABCDA#\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class LanguageDetector(L.LightningModule):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.train_loss = []\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.rnn = RNN(hidden_size, vocab_size)\n",
    "        self.dense = nn.Linear(hidden_size, vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, x, h_prev=None):\n",
    "        hiddens = self.rnn(x, h_prev)\n",
    "        out = self.dense(hiddens.squeeze(dim=-1))\n",
    "\n",
    "        return out, hiddens[-1]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x[0], y[0]  # remove batch\n",
    "        x = F.one_hot(x, self.vocab_size).float()\n",
    "        out = self.forward(x)[0]\n",
    "        loss = nn.CrossEntropyLoss()(out, y)\n",
    "\n",
    "        self.train_loss.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_acc = sum(self.train_loss) / len(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        self.print(f\"Epoch {self.current_epoch} Training Loss: {avg_train_acc:.5f}\")\n",
    "        print(\"------------------------------------\")\n",
    "        print(self.generate(\"A\"))\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    def generate(self, start=\"A\", max_len=100):\n",
    "        ret = start\n",
    "\n",
    "        last_hidden = torch.zeros(\n",
    "            size=(self.hidden_size, 1), dtype=torch.float32, device=self.device\n",
    "        )\n",
    "\n",
    "        for t in range(max_len):\n",
    "            input = F.one_hot(\n",
    "                str_to_tensor(ret[-1], device=self.device), vocab_size\n",
    "            ).float()\n",
    "            out, last_hidden = self.forward(input, last_hidden)\n",
    "            out = torch.softmax(out, dim=-1)\n",
    "            next = torch.multinomial(out, num_samples=1).squeeze()\n",
    "            input = F.one_hot(next, vocab_size).float()\n",
    "            ret = ret + idx_to_char[next.item()]\n",
    "\n",
    "            if ret[-1] == \"#\":\n",
    "                break\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return opt\n",
    "\n",
    "\n",
    "hidden_size = 7\n",
    "pattern = \"ABBCDA\"\n",
    "\n",
    "model = LanguageDetector(7, vocab_size)\n",
    "model.generate()\n",
    "\n",
    "train_dataset = TextSampler(pattern=pattern, vocab_size=vocab_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=12)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=100, accelerator=\"cpu\")\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCBABCABBCCA\n",
      "ADCCBDBBBCAAABBCDA\n",
      "ABDCDACDAABBCDA\n",
      "ABDDAABBCDA\n",
      "ACCBCDABBCDA\n",
      "ADABCAACBABBCDA\n",
      "AADDCDABBCDA\n",
      "AACBACCCCABBCDA\n",
      "ACDBBBBABBCDA\n",
      "ACBDABBCDBABBCDA\n",
      "ADDCBBACAABBCDA\n",
      "AAADDCDDDCDBABBCDA\n",
      "ACBBAABBCDA\n",
      "ABABBADABBCDA\n",
      "ADABADDDABBCDA\n",
      "ADDCDCAAAABBCDA\n",
      "ABDDABCDDBACABBCDA\n",
      "ADBCCAABBCDA\n",
      "AABBBAACBBCDA\n",
      "ADCCDADAABBCDA\n",
      "ABBCABBCDA\n",
      "ABCADBBCDADADBBCDA\n",
      "ADBDBBBBCDABABBCDA\n",
      "AAADABABBCDA\n",
      "ADDABCABABBCDA\n",
      "AADDABBCDA\n",
      "ADABCBBCCABBCDA\n",
      "ABCDACBBBCDCBBCDA\n",
      "ADDACADABBCDA\n",
      "ABABBCCBABBCDA\n",
      "AABABCCAABABBCDA\n",
      "AADBABADBCABBCDA\n",
      "ABACAAABBCDA\n",
      "ACACBABBBCDABCDA\n",
      "ACBDBBCDCADABBCDA\n",
      "ACDBCABBCDA\n",
      "ADCDDDACBABBCDA\n",
      "ABACDADAAAABDDABBCDA\n",
      "AADCABBAABBCDA\n",
      "ABCCAADBBCDACBBCDA\n",
      "ACACAADACBABBCDA\n",
      "ADADCDACDACAABBCDA\n",
      "AABDCAABBCDA\n",
      "ACCAABBADABCDABBCDA\n",
      "ADAACACBCCABBCDA\n",
      "ADBDABACAABBCDA\n",
      "ABCCCCABABBCDA\n",
      "ADDBCDADADABBCDA\n",
      "ABDAABBCDDAABBCDA\n",
      "ACCDDACADABBCDA\n",
      "AABADBABCDDABABBCDA\n",
      "ADCADCBCAABCCABBCDA\n",
      "AABDCDDCCABBCDA\n",
      "ABAAAAABCDABBCDA\n",
      "ABBDBBCADBABABBCDA\n",
      "ACABBDABBCDA\n",
      "AACBADBAAAABBCDA\n",
      "AABACACACAAABBCDA\n",
      "ADACBBDABAABBCDA\n",
      "ADDDBCCCABBCDA\n",
      "ACCDADACCBCBBABBCDA\n",
      "ACBAABBDDBBDAABBCDA\n",
      "ACDDABADAABBCDA\n",
      "ABCBDCDCABBCDA\n",
      "ABDDBBBAABBCDA\n",
      "AADACAABBCAA\n",
      "ADBCDAABBCDA\n",
      "ADCDDDDAABBCDA\n",
      "ADDBABBDAABBCDA\n",
      "AACCDACADDABBCDB\n",
      "AABBBDAABBCDA\n",
      "ADABDCADCDDABBCDA\n",
      "AADCDBDABBCDA\n",
      "ACBACAABBDAABBCDA\n",
      "ACDDBABABBCDA\n",
      "AAACCABCABBCDA\n",
      "ABCDDABBCDA\n",
      "ABBBADABABCDA\n",
      "ACDBCCABBDABBCDA\n",
      "ABDCABBCDDABBCDA\n",
      "ADACBDCAABBCDA\n",
      "ABCBADCAABBCDA\n",
      "ADDBCAABBDABBCDA\n",
      "ADABCDBCCDAAAABBCDA\n",
      "ADABABBCDA\n",
      "ACBCCCBCCABABBCDA\n",
      "ABADBCCBDCCCBABBCDA\n",
      "ABCACBDAABBCDA\n",
      "AABBAACBABBCDA\n",
      "AABABABBCDA\n",
      "ABBCADBAABABBCDA\n",
      "ABDDBABCCADABBCDA\n",
      "ABCDCBCBBABBCDA\n",
      "ABBABBCCABBCDA\n",
      "ACDCABABBCDA\n",
      "ADAABBCAABBCDA\n",
      "ABBADBBBCDAABBCDA\n",
      "ABDDDBAABBCDA\n",
      "ACDACCBCABBDDABBCDA\n",
      "ABBCDBCBBCCBBBCDA\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in range(100):\n",
    "    gen = model.generate()[:-1]\n",
    "    print(gen)\n",
    "    cnt += int(gen[-len(pattern) :] == pattern)\n",
    "\n",
    "print(cnt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15c7f8ccc8574ea79f6070caf809b88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e980ecc0f8842689b3221a00718ecf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42d6827efc05462d8acb60f29d90d849": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50d5f476be1148c4a9502c3247ee5b49",
       "IPY_MODEL_6037cf11e7294a83bbb8a67268a2d106",
       "IPY_MODEL_769836f3b05b4e5ebf8df1a08d7e6f54"
      ],
      "layout": "IPY_MODEL_cf7687bfef6540b490cf183b0eeb4188"
     }
    },
    "50d5f476be1148c4a9502c3247ee5b49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c76d17d4f86485b8db168d9a66b9f5a",
      "placeholder": "​",
      "style": "IPY_MODEL_be09dfe1386b42c8a5a8e1027e11ed35",
      "value": "Epoch 99: 100%"
     }
    },
    "6037cf11e7294a83bbb8a67268a2d106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea3522dd3ef349f09bc19769078f7887",
      "max": 128,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e980ecc0f8842689b3221a00718ecf8",
      "value": 128
     }
    },
    "6c76d17d4f86485b8db168d9a66b9f5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769836f3b05b4e5ebf8df1a08d7e6f54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c7f8ccc8574ea79f6070caf809b88b",
      "placeholder": "​",
      "style": "IPY_MODEL_ee662183e28a4ad185379742c1ce5d09",
      "value": " 128/128 [00:02&lt;00:00, 43.72it/s, v_num=13]"
     }
    },
    "be09dfe1386b42c8a5a8e1027e11ed35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf7687bfef6540b490cf183b0eeb4188": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "ea3522dd3ef349f09bc19769078f7887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee662183e28a4ad185379742c1ce5d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
